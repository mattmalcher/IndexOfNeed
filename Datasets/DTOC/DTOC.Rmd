


# Get DTOC Data
```{r Read DTOC URL's}
# Open a connection to DTOC files and read the lines into a char vector.
fileName <- "DTOC Files"
con <- file(fileName,open="r")
URLs <- readLines(con)
close(con)
rm(fileName,con)
```

```{r Get URLs from list}

URLs<-URLs[grep('http',URLs)] #get all lines starting with http

```

## Download Files
```{r Make Directory}
#Make the DTOC_CSVs directory if it does not exist
path<-"DTOC_CSVs"
dir.create(file.path(path), showWarnings = FALSE)

```

```{r Download Files}
#For each URL in our list
for(u in URLs){
  
  #Get filename from end of url and generate path for file
  fname=file.path(path,tail(strsplit(u,'/')[[1]], n=1))
  
  #if the filename (at the path) exists
  if(!file.exists(fname)){
    
      print(fname) # Print the name
    
      res <- try(download.file(u, destfile=fname, method="auto")) #try and download it
  }
}

rm(fname,u) # tidy up loop variables

```

## Read Files 
Here we read the files we have downloaded in. Note that the URL list is used again rather than just reading in all the files in the directory.
```{r Read in Downloaded Files}

# Note - Headers are inconsistent across files so here we provide a single set
# Assumption is that the data columns are the same.
headers<-c("Year","Period Name","Provider Parent Org Code","Provider Parent Name","Provider Org Code","Provider Org Name","Local Authority Code","Local Authority Name","Acute Or Non Acute Description","Reason For Delay","NHS A SUM","NHS B SUM","Social Care A SUM","Social Care B SUM","Both A SUM","Both B SUM")

#Get filename from end of url and generate path for file
f_names=sapply(strsplit(URLs,'/'),tail, n=1) #Alternate: # file_names <- dir(path)

#Read all of the files into a data frame

DTOC <- do.call(rbind, #Row bind the output of:
                  lapply(paste(path,f_names,sep='/'), #lapply, calling read.csv for every item in f_names
                                  read.csv,
                                    skip=7,           # Skipping the 7 header rows
                                    col.names=headers) # And using the headers as defined above
                )

rm(f_names, headers) # tidy up
```

## Tidy Dates
Recode Financial Year & Month to a proper date field
```{r}
require(dplyr); require(lubridate)

# Create list of months which for financial Year 20AA - BB fall in next year BB
endmonths<-c("JANUARY", "FEBRUARY", "MARCH")

#Get Year AA
DTOC$Date<-recode_factor(DTOC$Year,`2017-18`="2017",`2016-17`="2016", `2015-16`="2015")

#Depending on the period, create a date field (go from financial year -> year). Day set to 1st of the month
DTOC<-mutate(DTOC, 
             Date = case_when( Period.Name %in% endmonths ~ dmy(paste("01",Period.Name, as.numeric(as.character(Date))+1)),
                               TRUE                       ~ dmy(paste("01",Period.Name,Date))
                               )
             )
  

rm(endmonths)#tidy up

# Show the year and periods next to the assigned dates as a check
# DTOC %>% select(Year,Period.Name,Date) %>% distinct %>% arrange(Date)
```
## Tidy LA Codes

<!-- TODO - need to tidy up codes for bedfordshire and cheshire - see snippet debug join -->

## Reshape DTOC Data
Aggregate Social Care DTOC by Year
```{r}
require(reshape2)
DTOCTable<- dcast(DTOC, 
                  Local.Authority.Code ~ Date, 
                  value.var =  "Social.Care.B.SUM",
                  fun.aggregate = sum)
```

#Get LA to ODS Lookup
DTOC data is mapped via HSCIC Codes, Sometimes called IC Codes, Sometimes called ODS codes or Upper Tier LA Codes
best lookup I can find is:
http://www.gov.uk/government/uploads/system/uploads/attachment_data/file/495268/COVER_Q15_1_StatisticalTables_v5.xlsx

```{r}
require(readxl); require(dplyr)

#Define URL for lookup of LA ODS codes to ONS codes
lookupfile<-"http://www.gov.uk/government/uploads/system/uploads/attachment_data/file/495268/COVER_Q15_1_StatisticalTables_v5.xlsx"

#Get filename from end of URL
fname=file.path(tail(strsplit(lookupfile,'/')[[1]], n=1))

if(!file.exists(fname)){
    
      try(download.file(lookupfile, destfile=fname, method="auto", mode='wb')) #try and download it
}

#Read lookup from file
lookup <- read_xlsx(fname,sheet="12m_UT LA", range="E3:H154", col_names = c("ONS","ODS","PCT","Name"), col_types = "text") %>% select(-PCT, -Name)

```

# Get Population Estimates
Mid 2016 population estimates from the ONS
```{r}
require(readxl); require(dplyr)

#Define URL for Population Estimates
popurl <- "https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/populationestimatesforukenglandandwalesscotlandandnorthernireland/mid2016/ukmidyearestimates2016.xls"

#Get filename from end of URL
fname=file.path(tail(strsplit(popurl,'/')[[1]], n=1))

#Check if we have already downloaded it
if(!file.exists(fname)){
  
      #If not, try and download it
      try(download.file(popurl, destfile=fname, method="auto", mode='wb')) 
}

#Read population data in
popdata <- read_xls(fname,sheet="MYE2 - All", range="A5:CP445")

#Calculate a 65+ column
popdata %>% select(`65`:`90`) %>% rowSums(na.rm=TRUE) -> popdata$`65+`

#select only the ONS code & 65+ population column
popdata <- popdata %>% select(Code,"All ages",`65+`)
```


# Get Local Authority Boundaries

The ONS Open Geography Portal is one of the best places for UK shapefiles. You can get URL's for geojson's from the `API` dropdown on each page.

```{r}
require(dplyr); require(sf)

#Lower Tier Local Authority Boundaries
UKLA <- st_read(dsn = "https://opendata.arcgis.com/datasets/ae90afc385c04d869bc8cf8890bd1bcd_3.geojson")

# Country Data
Countries <- st_read(dsn = "https://opendata.arcgis.com/datasets/ffbb56e049b145a28720f0d817d6c197_0.geojson") 
st_geometry(Countries) <- NULL ; names(Countries) <-tolower(names(Countries)) # tidy up for join

#Upper Tier to Lower Tier Local Authority Lookup
UT2LT <- st_read(dsn = "https://opendata.arcgis.com/datasets/41828627a5ae4f65961b0e741258d210_0.geojson")
st_geometry(UT2LT) <- NULL ; names(UT2LT) <-tolower(names(UT2LT)) # tidy up for join

UKLA<-left_join(UKLA,Countries)
UKLA<-left_join(UKLA,UT2LT, by=c("lad17cd"="ltla17cd"))
```


# Join Data

##Join DTOC Table to Lookup
```{r}
DTOC_Coded<- left_join(DTOCTable, lookup, by=c("Local.Authority.Code"="ODS"))

DTOC_Coded %>% filter(is.na(ONS)) %>% select(Local.Authority.Code) -> err
#rm(DTOCTable,lookup)
```

Have a look at which LA's arent joining nicely
```{r Debug Join}
# DTOC %>% 
#   filter(Local.Authority.Code %in% err$Local.Authority.Code ) %>% 
#   select(Local.Authority.Code,Local.Authority.Name) %>%
#   distinct()
```


##Join Population Data to Geometry
```{r}
UKLA<- left_join(UKLA,popdata, by=c("lad17cd"="Code"))
```

##Join DTOC Data to Geometry
```{r}
MapData <- left_join(UKLA,DTOC_Coded,by=c("lad17cd"="ONS"))

MapData %>% 
  filter(ctry17nm=="England") %>%
  filter(is.na(Local.Authority.Code)) %>%
  select(lad17nm,lad17cd, Local.Authority.Code) -> err2
```

# Plot Map

Note - this requires a newer version of ggplot than that which is present on CRAN

```{r}
# Or the the development version from GitHub:
# install.packages("devtools")
#devtools::install_github("tidyverse/ggplot2")
```

```{r}
require(ggplot2);require(sf)

# equal intervals
ggplot() +
  geom_sf(data = MapData,
          aes(fill = cut_number(`All ages`, 5)),
          alpha = 0.8,
          colour = 'white',
          size = 0.3) +
  scale_fill_brewer(palette = "PuBu",
                    name = "Delayed Days") +
  labs(x = NULL, y = NULL,
       title = "DTOC",
       subtitle = "Jan 18",
       caption = "Contains OS data © Crown copyright and database right (2018)") +
  theme(line = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank(),
        panel.background = element_blank()) +
coord_sf(datum = NA)
ggsave("test.png", plot=last_plot(), dpi=300)
```